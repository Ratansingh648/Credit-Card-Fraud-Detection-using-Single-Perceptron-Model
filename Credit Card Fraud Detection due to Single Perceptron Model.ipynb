{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection using Single Perceptron Model\n",
    "\n",
    "#### Author:  Ratan Madankumar Singh    \n",
    "#### 3rd March, 2018\n",
    "\n",
    "In this document we have implemented a single neuron model to detect creditcard fraud detection. This data has been taken from the www.kaggle.com. This underlying dataset contains 284807 rows and 30 attribbutes defining the characteristics of the transaction. The fraudulent transaction is depicted by 1 and non-fraudulent transaction is denoted by 0. The biggest advantage of this dataset is that it is cleaned and preprocessed and therefore can directly used for analysis. Before applying the machine learning model, Let's create some basic nut-bolts to implement the machine learning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    y_hat = 1.0/(1+np.exp(-z))\n",
    "    return y_hat\n",
    "\n",
    "def weightInit(n):\n",
    "    W = np.random.random((1,n))*0.01\n",
    "    b = 0\n",
    "    return [W,b]\n",
    "\n",
    "def forwardPropagation(W,X,b):\n",
    "    z = np.dot(W,X)+b\n",
    "    y_hat = sigmoid(z)\n",
    "    return y_hat\n",
    "\n",
    "def computeCost(y_hat,y,lambd,W):\n",
    "    m = len(np.squeeze(y_hat))\n",
    "    epsilon = 1e-8\n",
    "    logprobs = np.multiply(np.log(y_hat+epsilon),y)+np.multiply(np.log(1-y_hat+epsilon),(1-y))\n",
    "    cost = -np.sum(logprobs)/m\n",
    "    return cost\n",
    "\n",
    "    J = np.sum((y_hat-y)**2) + lambd*np.sum(np.array(W)**2)\n",
    "    return J\n",
    "\n",
    "def computeGradient(y_hat,y,X,lambd,W):\n",
    "    m = len(np.squeeze(y_hat))\n",
    "    dW = np.dot((y_hat-y),X.T)/m + lambd*np.array(W)/m\n",
    "    db = np.sum(y_hat-y)/m\n",
    "    return [dW,db]\n",
    "\n",
    "def predictLogit(W,b,X):\n",
    "    z = np.dot(W,X)+b\n",
    "    y_hat = sigmoid(z)\n",
    "    return y_hat\n",
    "\n",
    "def accuracy(y_hat,y):\n",
    "    y_hat = (y_hat > 0.5)*1.0\n",
    "    acc = np.sum(y_hat == y)*100.0/len(np.squeeze(y))\n",
    "    return(acc)\n",
    "\n",
    "def initAdam(W,b):\n",
    "    Sw = np.zeros(W.shape)\n",
    "    Sb = 0\n",
    "    Vw = np.zeros(W.shape)\n",
    "    Vb = 0\n",
    "    return [Sw,Sb,Vw,Vb]\n",
    "\n",
    "def adam(W,b,dW,db,Sw,Sb,Vw,Vb,beta1,beta2,alpha,epsilon = 1e-8):\n",
    "    Sw = beta1*Sw + (1-beta1)*np.square(dW)\n",
    "    Sb = beta1*Sb + (1-beta1)*np.square(db)\n",
    "    Vw = beta2*Vw + (1-beta2)*dW\n",
    "    Vb = beta2*Vb + (1-beta2)*db\n",
    "    W = W - alpha*Vw/(np.sqrt(Sw)+epsilon)\n",
    "    b = b - alpha*Vb/(np.sqrt(Sb)+epsilon)\n",
    "    return [W,b,Sw,Sb,Vw,Vb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is stored in my local machine. Let's load it and shuffle it randomly. After random shuffling, I am splitting it into training and testing dataset with 200000 observations in training dataset and 84807 rows in test dataset. To optimize the cost function, I have used Adam Algorithm which converges faster than the batch gradient descent algorithm and does not slow down in horse-saddle region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of file is 284807 rows and 31 columns\n",
      "Iteration Number - 0 Cost::18.235161709\n",
      "Iteration Number - 1000 Cost::0.0350230249403\n",
      "Iteration Number - 2000 Cost::0.0425708822919\n",
      "Iteration Number - 3000 Cost::0.0182366973248\n",
      "Iteration Number - 4000 Cost::0.0332493187622\n",
      "Iteration Number - 5000 Cost::0.0316415030608\n",
      "Iteration Number - 6000 Cost::0.0246549501354\n",
      "Iteration Number - 7000 Cost::0.017996047407\n",
      "Iteration Number - 8000 Cost::0.0331716918875\n",
      "Iteration Number - 9000 Cost::0.0314965932636\n",
      "Iteration Number - 10000 Cost::0.0192749779762\n",
      "Iteration Number - 11000 Cost::0.0317312487701\n",
      "Iteration Number - 12000 Cost::0.0198944613009\n",
      "Iteration Number - 13000 Cost::0.0327106688306\n",
      "Iteration Number - 14000 Cost::0.0299536099718\n",
      "Training the model is completed ::\n"
     ]
    }
   ],
   "source": [
    "DataFrame = pd.read_csv('C:\\\\Users\\\\Ratan Singh\\\\Documents\\\\R Markdown Files\\\\credit card fraud\\\\creditcard.csv')\n",
    "print(\"Dimensions of file is \"+str(DataFrame.shape[0])+\" rows and \"+str(DataFrame.shape[1])+\" columns\" )\n",
    "\n",
    "order = np.random.permutation(DataFrame.shape[0])\n",
    "DataFrame = DataFrame.iloc[order,:]\n",
    "cutOff = 200000\n",
    "\n",
    "X_trainData = DataFrame.iloc[range(0,cutOff),range(0,30)].T\n",
    "X_testData = DataFrame.iloc[range(cutOff,DataFrame.shape[0]),range(0,30)].T\n",
    "\n",
    "Y_trainData = (DataFrame.iloc[range(0,cutOff),30].T).values.reshape(1,cutOff)\n",
    "Y_testData = (DataFrame.iloc[range(cutOff,DataFrame.shape[0]),30].T).values.reshape(1,DataFrame.shape[0]-cutOff)\n",
    "\n",
    "Y_trainData = (Y_trainData > 0)*1.0\n",
    "Y_testData = (Y_testData > 0)*1.0\n",
    "\n",
    "num_iter = 15000\n",
    "lambd = 0.1\n",
    "alpha = 0.1\n",
    "beta1 = 0.99\n",
    "beta2 = 0.9\n",
    "\n",
    "J = []\n",
    "[W,b] = weightInit(30)\n",
    "\n",
    "[Sw,Sb,Vw,Vb] = initAdam(W,b)\n",
    "\n",
    "\n",
    "for i in range(0,num_iter):\n",
    "    \n",
    "    y_hat = forwardPropagation(W,X_trainData,b)\n",
    "    if(i%1000 == 0):\n",
    "        print(\"Iteration Number - \"+str(i)+ \" Cost::\"+str(computeCost(y_hat,Y_trainData,lambd,W)))\n",
    "        J.append(computeCost(y_hat,Y_trainData,lambd,W))\n",
    "            \n",
    "    [dW,db] = computeGradient(y_hat,Y_trainData,X_trainData,lambd,W)\n",
    "    [W,b,Sw,Sb,Vw,Vb] = adam(W,b,dW,db,Sw,Sb,Vw,Vb,beta1,beta2,alpha)\n",
    "        \n",
    "print(\"Training the model is completed ::\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are getting Cost as NaN as the cost is very low during each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy due to Adam :: 99.7087504569\n"
     ]
    }
   ],
   "source": [
    "y_hat = predictLogit(W,b,X_testData)\n",
    "print(\"Accuracy due to Adam :: \"+str(accuracy(y_hat,Y_testData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(J))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can observe that our model has an accuracy of 99.7%  on the test dataset. This all is attributed to the cleaned data that was given in dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
